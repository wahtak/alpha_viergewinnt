#!/bin/env python
import click
import logging

import numpy as np
import matplotlib.pyplot as plt

from alpha_viergewinnt.game.board import Player
from alpha_viergewinnt.game.tictactoe import Tictactoe
from alpha_viergewinnt.game.viergewinnt import Viergewinnt
from alpha_viergewinnt.player.alpha_player.factory import \
    create_generic_estimator, create_mlp_estimator, create_alpha_player, create_alpha_trainer
from alpha_viergewinnt.match import CompetitionMatch, TrainingMatch

logger = logging.getLogger(__name__)

GAME_FACTORIES = {'tictactoe': Tictactoe, 'viergewinnt': Viergewinnt}
ESTIMATOR_FACTORIES = {'generic': create_generic_estimator, 'mlp': create_mlp_estimator}


@click.command()
@click.option('--game', required=True, type=click.Choice(GAME_FACTORIES.keys()), help='Game to be trained')
@click.option(
    '--estimator', required=True, type=click.Choice(ESTIMATOR_FACTORIES.keys()), help='Estimator to be trained')
@click.option('--mcts-steps', required=True, type=int, help='Number of MCTS steps per move')
@click.option('--training-iterations', type=int, default=0, help='Number of training cycles to be run')
@click.option('--comparison-iterations', type=int, default=0, help='Number of comparison cycles to be run')
@click.option('--plot', type=bool, default=False, help='Plot loss during training')
def cmd(game, estimator, mcts_steps, training_iterations, comparison_iterations, plot):
    create_game = GAME_FACTORIES[game]
    create_training_estimator = ESTIMATOR_FACTORIES[estimator]

    game = create_game()
    training_estimator = create_training_estimator(game=game)
    training_estimator.load()
    trainer_x = create_alpha_trainer(estimator=training_estimator, player=Player.X, mcts_steps=mcts_steps)

    if training_iterations > 0:
        logger.info('Starting training')

        trainer_o = create_alpha_trainer(estimator=training_estimator, player=Player.O, mcts_steps=mcts_steps)
        training_match = TrainingMatch(game=game, players={Player.X: trainer_x, Player.O: trainer_o})

        losses = []

        loss_plotter = LossPlotter(window_size=128)
        for i in range(training_iterations):
            loss = training_match.train()
            losses.append(loss)
            if i % 10 == 0:
                if plot:
                    loss_plotter.plot(losses)
                training_estimator.save()

    if comparison_iterations > 0:
        logger.info('Starting comparison')

        player_x = create_alpha_player(estimator=training_estimator, player=Player.X, mcts_steps=mcts_steps)
        comparison_estimator = create_generic_estimator(game)
        player_o = create_alpha_player(estimator=comparison_estimator, player=Player.O, mcts_steps=mcts_steps)

        comparison_match = CompetitionMatch(game=game, players={Player.X: player_x, Player.O: player_o})

        results = comparison_match.compare(comparison_iterations)
        outcome_percentages = tuple(
            100 * np.array([results[Player.X], results[Player.O], results[None]]) / comparison_iterations)
        logger.info(
            'Comparison results: %.2f%% player X wins, %.2f%% player O wins, %.2f%% draws.' % outcome_percentages)


class LossPlotter(object):
    def __init__(self, window_size):
        self.window_size = window_size
        plt.ion()
        plt.show()

    def plot(self, loss):
        plt.clf()
        plt.plot(loss)
        plt.plot(self._filter_loss(loss))
        plt.pause(0.001)

    def _filter_loss(self, loss):
        padded_loss = np.concatenate(
            [np.full(self.window_size // 2, loss[0]),
             np.array(loss),
             np.full(self.window_size // 2, loss[-1])])
        filter_kernel = np.ones((self.window_size, )) / self.window_size
        return np.convolve(padded_loss, filter_kernel, mode='valid')


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    cmd()
