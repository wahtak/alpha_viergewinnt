#!/bin/env python
import click
import logging

import numpy as np
import matplotlib.pyplot as plt

from alpha_viergewinnt.game.board import Player
from alpha_viergewinnt.game import tictactoe, viergewinnt
from alpha_viergewinnt.player.alpha_player import \
    AlphaPlayer, AlphaTrainer, GenericEstimator, MlpEstimator, Evaluator
from alpha_viergewinnt.match import ComparisonMatch, TrainingMatch

logger = logging.getLogger(__name__)

GAME_FACTORIES = {
    'tictactoe': (tictactoe.Game, tictactoe.WinCondition, tictactoe.DrawCondition),
    'viergewinnt': (viergewinnt.Game, viergewinnt.WinCondition, viergewinnt.DrawCondition)
}

ESTIMATOR_FACTORIES = {'generic': GenericEstimator, 'mlp': MlpEstimator}


@click.command()
@click.option('--game', required=True, type=click.Choice(GAME_FACTORIES.keys()), help='Game to be trained')
@click.option(
    '--estimator', required=True, type=click.Choice(ESTIMATOR_FACTORIES.keys()), help='Estimator to be trained')
@click.option('--mcts-steps', required=True, type=int, help='Number of MCTS steps per move')
@click.option('--training-iterations', type=int, default=0, help='Number of training cycles to be run')
@click.option('--comparison-iterations', type=int, default=0, help='Number of comparison cycles to be run')
def cmd(game, estimator, mcts_steps, training_iterations, comparison_iterations):
    filename = 'resources/{}_{}'.format(game, estimator)

    Game, WinCondition, DrawCondition = GAME_FACTORIES[game]
    TrainingEstimator = ESTIMATOR_FACTORIES[estimator]

    game = Game()
    win_condition_x = WinCondition(Player.X)
    win_condition_o = WinCondition(Player.O)
    draw_condition = DrawCondition()

    training_estimator = TrainingEstimator(board_size=game.board_size, actions=game.get_all_moves(), filename=filename)

    training_evaluator_x = Evaluator(
        estimator=training_estimator,
        player=Player.X,
        opponent=Player.O,
        win_condition=win_condition_x,
        loss_condition=win_condition_o,
        draw_condition=draw_condition)
    trainer_x = AlphaTrainer(training_evaluator_x, mcts_steps)

    if training_iterations > 0:
        logger.info('Starting training')

        training_evaluator_o = Evaluator(
            estimator=training_estimator,
            player=Player.O,
            opponent=Player.X,
            win_condition=win_condition_o,
            loss_condition=win_condition_x,
            draw_condition=draw_condition)
        trainer_o = AlphaTrainer(training_evaluator_o, mcts_steps)

        training_match = TrainingMatch(
            game=game,
            players={Player.X: trainer_x,
                     Player.O: trainer_o},
            win_conditions={Player.X: win_condition_x,
                            Player.O: win_condition_o},
            draw_condition=draw_condition)

        training_estimator.load()

        losses = []
        plt.ion()
        plt.show()
        for i in range(training_iterations):
            loss = training_match.train()
            losses.append(loss)
            if i % 10 == 0:
                plt.clf()
                plt.plot(losses)
                plt.plot(
                    np.convolve(([losses[0]] * 16) + losses + ([losses[-1]] * 16), np.ones((32, )) / 32, mode='valid'))
                plt.pause(0.001)
                training_estimator.save()

    if comparison_iterations > 0:
        logger.info('Starting comparison')

        player_x = AlphaPlayer(training_evaluator_x, mcts_steps)

        comparison_estimator = GenericEstimator(board_size=game.board_size, actions=game.get_all_moves(), filename=None)
        comparison_evaluator_o = Evaluator(
            estimator=comparison_estimator,
            player=Player.O,
            opponent=Player.X,
            win_condition=win_condition_o,
            loss_condition=win_condition_x,
            draw_condition=draw_condition)
        player_o = AlphaPlayer(comparison_evaluator_o, mcts_steps)

        comparison_match = ComparisonMatch(
            game=game,
            players={Player.X: player_x,
                     Player.O: player_o},
            win_conditions={Player.X: win_condition_x,
                            Player.O: win_condition_o},
            draw_condition=draw_condition)

        results = comparison_match.compare(comparison_iterations)
        outcome_percentages = tuple(
            100 * np.array([results[Player.X], results[Player.O], results[None]]) / comparison_iterations)
        logger.info(
            'Comparison results: %.2f%% player X wins, %.2f%% player O wins, %.2f%% draws.' % outcome_percentages)


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    cmd()
