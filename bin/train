#!/bin/env python
import click
import logging

import numpy as np
import matplotlib.pyplot as plt

from alpha_viergewinnt.game.board import Player
from alpha_viergewinnt.game import tictactoe, viergewinnt
from alpha_viergewinnt.player.alpha_player import \
    AlphaPlayer, AlphaTrainer, GenericEstimator, MlpEstimator, Evaluator
from alpha_viergewinnt.match import ComparisonMatch, TrainingMatch

logger = logging.getLogger(__name__)

GAME_FACTORIES = {
    'tictactoe': tictactoe.Game,
    'viergewinnt': viergewinnt.Game,
}

ESTIMATOR_FACTORIES = {'generic': GenericEstimator, 'mlp': MlpEstimator}


@click.command()
@click.option('--game', required=True, type=click.Choice(GAME_FACTORIES.keys()), help='Game to be trained')
@click.option(
    '--estimator', required=True, type=click.Choice(ESTIMATOR_FACTORIES.keys()), help='Estimator to be trained')
@click.option('--mcts-steps', required=True, type=int, help='Number of MCTS steps per move')
@click.option('--training-iterations', type=int, default=0, help='Number of training cycles to be run')
@click.option('--comparison-iterations', type=int, default=0, help='Number of comparison cycles to be run')
def cmd(game, estimator, mcts_steps, training_iterations, comparison_iterations):
    filename = 'resources/{}_{}'.format(game, estimator)

    Game = GAME_FACTORIES[game]
    TrainingEstimator = ESTIMATOR_FACTORIES[estimator]

    game = Game()
    training_estimator = TrainingEstimator(board_size=game.board_size, actions=game.get_all_moves(), filename=filename)
    training_estimator.load()
    trainer_x = AlphaTrainer(Evaluator(estimator=training_estimator, player=Player.X), mcts_steps)

    if training_iterations > 0:
        logger.info('Starting training')

        trainer_o = AlphaTrainer(Evaluator(estimator=training_estimator, player=Player.O), mcts_steps)
        training_match = TrainingMatch(game=game, players={Player.X: trainer_x, Player.O: trainer_o})

        losses = []
        plt.ion()
        plt.show()
        for i in range(training_iterations):
            loss = training_match.train()
            losses.append(loss)
            if i % 10 == 0:
                plt.clf()
                plt.plot(losses)
                plt.plot(
                    np.convolve(([losses[0]] * 16) + losses + ([losses[-1]] * 16), np.ones((32, )) / 32, mode='valid'))
                plt.pause(0.001)
                training_estimator.save()

    if comparison_iterations > 0:
        logger.info('Starting comparison')

        player_x = AlphaPlayer(Evaluator(estimator=training_estimator, player=Player.X), mcts_steps)
        comparison_estimator = GenericEstimator(board_size=game.board_size, actions=game.get_all_moves(), filename=None)
        player_o = AlphaPlayer(Evaluator(estimator=comparison_estimator, player=Player.O), mcts_steps)

        comparison_match = ComparisonMatch(game=game, players={Player.X: player_x, Player.O: player_o})

        results = comparison_match.compare(comparison_iterations)
        outcome_percentages = tuple(
            100 * np.array([results[Player.X], results[Player.O], results[None]]) / comparison_iterations)
        logger.info(
            'Comparison results: %.2f%% player X wins, %.2f%% player O wins, %.2f%% draws.' % outcome_percentages)


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    cmd()
