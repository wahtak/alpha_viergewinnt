#!/bin/env python
import logging

import click
import numpy as np
import matplotlib.pyplot as plt

from alpha_viergewinnt.game.board import Player
from alpha_viergewinnt.game.tictactoe import Tictactoe
from alpha_viergewinnt.game.viergewinnt import Viergewinnt
from alpha_viergewinnt.agent.alpha.factory import \
    create_generic_estimator, create_mlp_estimator, create_alpha_agent, create_alpha_trainer
from alpha_viergewinnt.match import CompetitionMatch, TrainingMatch
from alpha_viergewinnt.logging import ValueLogger, set_logger, log

logger = logging.getLogger(__name__)

GAME_FACTORIES = {'tictactoe': Tictactoe, 'viergewinnt': Viergewinnt}
ESTIMATOR_FACTORIES = {'generic': create_generic_estimator, 'mlp': create_mlp_estimator}


@click.command()
@click.option('--game', required=True, type=click.Choice(GAME_FACTORIES.keys()), help='Game to be trained')
@click.option(
    '--estimator', required=True, type=click.Choice(ESTIMATOR_FACTORIES.keys()), help='Estimator to be trained')
@click.option('--mcts-steps', required=True, type=int, help='Number of MCTS steps per move')
@click.option('--num-training-games', type=int, default=0, help='Number of training games per epoch')
@click.option('--num-comparison-games', type=int, default=0, help='Number of comparison games per epoch')
@click.option('--disable-plotting', type=bool, default=False, help='Disable plotting loss and score')
@click.option(
    '--reload-last-epoch', type=bool, default=True, help='Reload parameters from last epoch if score gets worse')
@click.option('--num-epochs', type=int, default=-1, help='Number of epochs to train, default=unlimited, 0=only compare')
def cmd(game, estimator, mcts_steps, num_training_games, num_comparison_games, disable_plotting, reload_last_epoch,
        num_epochs):
    create_game = GAME_FACTORIES[game]
    create_estimator = ESTIMATOR_FACTORIES[estimator]

    game = create_game()

    # load possibly pre-existing parameters
    trainer_estimator = create_estimator(game=game)
    params_filename = '{}_{}.params'.format(trainer_estimator.__class__.__name__, game.__class__.__name__)
    trainer_estimator.load(params_filename)

    if num_epochs == 0:
        compare(game, trainer_estimator, mcts_steps, num_comparison_games)
        return

    # start with same parameters as trainer
    opponent_estimator = create_estimator(game=game)
    opponent_estimator.load(params_filename)

    value_logger = ValueLogger()
    value_logger.add_plot(name='loss', xlabel='game', filter_size=128)
    value_logger.add_plot(name='score', xlabel='epoch')
    set_logger(value_logger)

    epoch = 1
    best_score = -1
    while num_epochs < 0 or epoch < num_epochs:
        logger.info('Epoch %d' % epoch)
        train_epoch(game, trainer_estimator, opponent_estimator, mcts_steps, num_training_games)

        score = compare(game, trainer_estimator, mcts_steps, num_comparison_games)

        score_increase = score - best_score
        logger.info('Score increase since last epoch: %.4f%%.' % score_increase)
        if reload_last_epoch is True and score_increase < 0:
            logger.info('Loading last parameters.')
            trainer_estimator.load(params_filename)
        else:
            logger.info('Saving parameters and updating opponent parameters.')
            trainer_estimator.save(params_filename)
            opponent_estimator.load(params_filename)
            best_score = score

        epoch += 1


def train_epoch(game, trainer_estimator, opponent_estimator, mcts_steps, num_games):
    logger.info('Starting training')

    trainer_x = create_alpha_trainer(estimator=trainer_estimator, player=Player.X, mcts_steps=mcts_steps)
    trainer_o = create_alpha_trainer(estimator=trainer_estimator, player=Player.O, mcts_steps=mcts_steps)
    opponent_x = create_alpha_agent(estimator=opponent_estimator, player=Player.X, mcts_steps=mcts_steps)
    opponent_o = create_alpha_agent(estimator=opponent_estimator, player=Player.O, mcts_steps=mcts_steps)

    for i in range(num_games // 2):
        # train once each for different starting player
        training_match = TrainingMatch(game=game, agents={Player.X: trainer_x, Player.O: opponent_x})
        log(loss=training_match.train())

        training_match = TrainingMatch(game=game, agents={Player.O: trainer_o, Player.X: opponent_o})
        log(loss=training_match.train())


def compare(game, trainer_estimator, mcts_steps, num_games):
    if num_games == 0:
        return -1

    logger.info('Starting comparison')

    trainer_x = create_alpha_agent(estimator=trainer_estimator, player=Player.X, mcts_steps=mcts_steps)
    trainer_o = create_alpha_agent(estimator=trainer_estimator, player=Player.O, mcts_steps=mcts_steps)
    comparison_x = create_alpha_agent(estimator=create_generic_estimator(game), player=Player.X, mcts_steps=mcts_steps)
    comparison_o = create_alpha_agent(estimator=create_generic_estimator(game), player=Player.O, mcts_steps=mcts_steps)

    # compare for different starting player
    match = CompetitionMatch(game=game, agents={Player.X: trainer_x, Player.O: comparison_o})
    results = match.compare(num_games // 2)
    num_wins = results[Player.X]
    num_losses = results[Player.O]
    num_draws = results[None]

    match = CompetitionMatch(game=game, agents={Player.O: trainer_o, Player.X: comparison_x})
    results = match.compare(num_games // 2)
    num_wins += results[Player.O]
    num_losses += results[Player.X]
    num_draws += results[None]

    score = (num_wins - num_losses) / num_games
    logger.info('Comparison score: %.4f%% ' % score)
    result_percentages = tuple(100 * np.array([num_wins, num_losses, num_draws]) / num_games)
    logger.info('Comparison percentages: %.2f%% player X wins, %.2f%% player O wins, %.2f%% draws' % result_percentages)
    log(score=score)

    return score


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    cmd()
