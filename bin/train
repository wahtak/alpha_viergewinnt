#!/bin/env python
import click
import logging

import numpy as np
import matplotlib.pyplot as plt

from alpha_viergewinnt.game.board import Player
from alpha_viergewinnt.game.tictactoe import Tictactoe
from alpha_viergewinnt.game.viergewinnt import Viergewinnt
from alpha_viergewinnt.agent.alpha.factory import \
    create_generic_estimator, create_mlp_estimator, create_alpha_agent, create_alpha_trainer
from alpha_viergewinnt.match import CompetitionMatch, TrainingMatch
from alpha_viergewinnt.logging import ValueLogger, set_logger, log

logger = logging.getLogger(__name__)

GAME_FACTORIES = {'tictactoe': Tictactoe, 'viergewinnt': Viergewinnt}
ESTIMATOR_FACTORIES = {'generic': create_generic_estimator, 'mlp': create_mlp_estimator}


@click.command()
@click.option('--game', required=True, type=click.Choice(GAME_FACTORIES.keys()), help='Game to be trained')
@click.option(
    '--estimator', required=True, type=click.Choice(ESTIMATOR_FACTORIES.keys()), help='Estimator to be trained')
@click.option('--mcts-steps', required=True, type=int, help='Number of MCTS steps per move')
@click.option('--num-training-games', type=int, default=0, help='Number of training games per epoch')
@click.option('--num-comparison-games', type=int, default=0, help='Number of comparison games per epoch')
@click.option('--disable-plotting', type=bool, default=False, help='Disable plotting loss and score')
@click.option(
    '--reload-last-epoch', type=bool, default=True, help='Reload parameters from last epoch if score gets worse')
@click.option('--num-epochs', type=int, default=-1, help='Number of epochs to train, default=unlimited, 0=only compare')
def cmd(game, estimator, mcts_steps, num_training_games, num_comparison_games, disable_plotting, reload_last_epoch,
        num_epochs):
    create_game = GAME_FACTORIES[game]
    create_training_estimator = ESTIMATOR_FACTORIES[estimator]

    game = create_game()
    training_estimator = create_training_estimator(game=game)
    params_filename = '{}_{}.params'.format(training_estimator.__class__.__name__, game.__class__.__name__)
    training_estimator.load(params_filename)

    if num_epochs == 0:
        compare(game, training_estimator, mcts_steps, num_comparison_games)
        return

    value_logger = ValueLogger()
    value_logger.add_plot(name='loss', xlabel='game', filter_size=128)
    value_logger.add_plot(name='score', xlabel='epoch')
    set_logger(value_logger)

    epoch = 1
    best_score = -1
    while num_epochs < 0 or epoch < num_epochs:
        logger.info('Epoch %d' % epoch)
        train_epoch(game, training_estimator, mcts_steps, num_training_games)

        score = compare(game, training_estimator, mcts_steps, num_comparison_games)

        score_increase = score - best_score
        logger.info('Score increase since last epoch: %.4f%%.' % score_increase)
        if reload_last_epoch is True and score_increase < 0:
            logger.info('Loading last parameters.')
            training_estimator.load(params_filename)
        else:
            logger.info('Saving parameters.')
            training_estimator.save(params_filename)
            best_score = score

        epoch += 1


def train_epoch(game, training_estimator, mcts_steps, num_games):
    logger.info('Starting training')

    trainer_x = create_alpha_trainer(estimator=training_estimator, player=Player.X, mcts_steps=mcts_steps)
    trainer_o = create_alpha_trainer(estimator=training_estimator, player=Player.O, mcts_steps=mcts_steps)
    training_match = TrainingMatch(game=game, agents={Player.X: trainer_x, Player.O: trainer_o})

    for i in range(num_games):
        loss = training_match.train()
        log(loss=loss)


def compare(game, training_estimator, mcts_steps, num_games):
    if num_games == 0:
        return -1

    logger.info('Starting comparison')

    agent_x = create_alpha_agent(estimator=training_estimator, player=Player.X, mcts_steps=mcts_steps)
    comparison_estimator = create_generic_estimator(game)
    agent_o = create_alpha_agent(estimator=comparison_estimator, player=Player.O, mcts_steps=mcts_steps)

    comparison_match = CompetitionMatch(game=game, agents={Player.X: agent_x, Player.O: agent_o})

    results = comparison_match.compare(num_games)

    num_wins = results[Player.X]
    num_losses = results[Player.O]
    num_draws = results[None]
    score = (num_wins - num_losses) / num_games
    logger.info('Comparison score: %.4f%% ' % score)
    result_percentages = tuple(100 * np.array([num_wins, num_losses, num_draws]) / num_games)
    logger.info('Comparison percentages: %.2f%% player X wins, %.2f%% player O wins, %.2f%% draws' % result_percentages)
    log(score=score)

    return score


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    cmd()
